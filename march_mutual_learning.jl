#### march exploration and exploitation model main file

module MarchMutualLearning

import Random

export InitialEnvironment, simulate

mutable struct StatesVec{T<:Integer}
    states::Vector{T}
end

mutable struct InitialEnvironment{T}
    agents::Vector{StatesVec{T}}
    reality::StatesVec{T}
end

function StatesVec(len::Integer, vals::Vector{T})::StatesVec{T} where T<:Integer
    StatesVec(rand(vals, len))
end

function InitialEnvironment(m::T, n::T) where T<:Integer ## do dodania generyczność
    InitialEnvironment(
        [StatesVec(m, [-1, 0, 1]) for i in 1:n],
        StatesVec(m, [-1, 1])
    )
end

function learn!(learner::StatesVec{T}, teacher::StatesVec{T}, p₁::Real) where T<:Integer
    idx = (teacher.states .!= T(0)) .& (rand(length(teacher.states)) .<= p₁)
    view(learner.states, idx)[:] .= view(teacher.states, idx)
end

function measure_compatibility(entity1::StatesVec{T}, entity2::StatesVec{T})::Integer where T<:Integer
    # if index_range != nothing
    #     return entity1.states[index_range] .== entity2.states[index_range]
    # end

    sum(entity1.states .== entity2.states)
end

function count_views(dim::Integer, agents::AbstractVector{StatesVec{T}}, vals::Vector{T})::Vector{Integer} where T<:Integer
    map(v->count(a->a.states[dim]==v, agents), vals)
end

function calc_change_probability(counted_views::Vector{Integer}, p₂::Real)::Real  where T<:Integer
    1 - (1 - p₂)^abs(counted_views[1] - counted_views[2])
end

function adjust!(org_code::StatesVec{T}, reality::StatesVec{T}, agents::AbstractVector{StatesVec{T}}, p₂::Real, vals::Vector{T}) where T<:Integer
    code_comp = measure_compatibility(org_code, reality)
    better_comp_agents = view(agents, map(a->measure_compatibility(a, reality), agents) .> code_comp)

    for i in eachindex(org_code.states)
        counted_views = count_views(i, better_comp_agents, vals)
        majority_view = vals[argmax(counted_views)]
        prob = majority_view == org_code.states[i] ? 0 : calc_change_probability(counted_views, p₂)

        if rand() < prob
           org_code.states[i] = majority_view
        end
    end
end

function update_mean(m::T, new_val::T, n::Integer) where T<:Real
    (m * n + new_val) / (n + 1)
end

function simulate(env::InitialEnvironment{T}, rep::V, p₁::U, p₂::U, p₃::U = U(0), p₄::U = U(0), eq_precision::V = 4) where {T<:Integer, U<:Real, V<:Integer}
    simulate(env, rep, fill(p₁, length(env.agents)), p₂, p₃, p₄, eq_precision, true)
end

function simulate(env::InitialEnvironment{T}, rep::V, p₁::Vector{U}, p₂::U, p₃::U = U(0), p₄::U = U(0), eq_precision::V = 4, homogenius::Bool = false) where {T<:Integer, U<:Real, V<:Integer}
    m, n = length(env.reality.states), length(env.agents)
    code_comp_mean, agents_comp_mean = 0.0, 0.0

    better_agents_comp_mean, worse_agents_comp_mean = 0.0, 0.0

    if (!homogenius)
        unique_p₁ = sort(unique(p₁))
        better_learning_agents_indexes = p₁ .== unique_p₁[end]

        n_ba = sum(better_learning_agents_indexes)
        n_wa = n - n_ba
    end

    for i in 1:rep
        org_code = StatesVec(fill(T(0), m))
        agents = deepcopy(env.agents)
        reality = deepcopy(env.reality)
        agents_comp = Vector{Real}()

        j = 1
        while j <= 3 || (agents_comp[j-1] != agents_comp[j-2] || agents_comp[j-1] != agents_comp[j-3])
            if p₄ > 0
                idx = rand(n) .< p₄
                view(reality.states, idx)[:] *= T(-1)
            end

            push!(agents_comp, round(sum(a->measure_compatibility(a, reality)/m, agents)/n, digits = eq_precision))
            for k in eachindex(agents)
                if rand() < p₃
                    agents[k] = StatesVec(m)
                end

                learn!(agents[k], org_code, p₁[k])
            end
            adjust!(org_code, reality, agents, p₂, [-1, 1])
            j += 1
        end

        code_comp_mean = update_mean(code_comp_mean, measure_compatibility(org_code, reality)/m, i - 1)
        agents_comp_mean = update_mean(agents_comp_mean, sum(a->measure_compatibility(a, reality)/m, agents)/n, i - 1)

        if (!homogenius)
            if (n_ba > 0)
                better_agents_comp_mean = update_mean(better_agents_comp_mean, sum(a->measure_compatibility(a, reality)/m, agents[better_learning_agents_indexes])/n_ba, i - 1)
            end

            if (n_wa > 0)
                worse_agents_comp_mean = update_mean(worse_agents_comp_mean, sum(a->measure_compatibility(a, reality)/m, agents[.!better_learning_agents_indexes])/n_wa, i - 1)
            end

        end
    end

    if (!homogenius)
        return code_comp_mean, agents_comp_mean, better_agents_comp_mean, worse_agents_comp_mean
    end

    code_comp_mean, agents_comp_mean
end

end
